# LoRa Fine-tuning Configuration
# Modify these settings to experiment with different configurations

[model]
# Choose your model (any HuggingFace model for classification)
model_name = "distilbert-base-uncased"
# Alternatives:
# model_name = "bert-base-uncased"
# model_name = "roberta-base"
# model_name = "microsoft/DialoGPT-medium"

[dataset]
# Dataset configuration
dataset_name = "imdb"
train_size = 5000
val_size = 1000
test_size = 1000
max_length = 512

# For other datasets, uncomment and modify:
# dataset_name = "ag_news"
# num_labels = 4
# dataset_name = "yelp_review_full"
# num_labels = 5

[lora]
# LoRa hyperparameters
r = 16                    # Rank (4, 8, 16, 32, 64)
lora_alpha = 32          # Scaling parameter (usually 2*r)
lora_dropout = 0.1       # Dropout rate (0.0 - 0.3)
target_modules = ["q_lin", "v_lin"]  # For DistilBERT

# For BERT/RoBERTa, use:
# target_modules = ["query", "value"]

# For GPT models, use:
# target_modules = ["c_attn"]

[training]
# Training hyperparameters
num_epochs = 3
batch_size = 8
learning_rate = 5e-5
weight_decay = 0.01
warmup_steps = 500

# Logging and evaluation
logging_steps = 100
eval_steps = 500
save_steps = 1000

[evaluation]
# Evaluation settings
eval_batch_size = 16
max_eval_samples = 1000

[paths]
# Output directories
output_dir = "./lora_finetuned_model"
logs_dir = "./logs"
results_file = "lora_results.json"
plot_file = "lora_comparison.png"